{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ff923d-7941-42d5-86e9-aafcb26dedff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import r2_score, precision_score, accuracy_score, f1_score, mean_squared_error, mean_absolute_error, recall_score, confusion_matrix\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d5022e-2b49-4d2c-b25c-2e919467370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "model = LogisticRegression(random_state=42, max_iter=4000)\n",
    "\n",
    "features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
    "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
    "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
    "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
    "       'smoothness error', 'compactness error', 'concavity error',\n",
    "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
    "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
    "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
    "       'worst concave points', 'worst symmetry', 'worst fractal dimension']\n",
    "\n",
    "target = 'target'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d45e74-0baf-4905-8666-621c98ad8845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.956, 'precision': 0.946, 'recall': 0.986, 'f1': 0.966}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 1: Write function that takes y_true, y_pred as inputs\n",
    "# Returns dictionary with keys: 'accuracy', 'precision', 'recall', 'f1'\n",
    "# Use sklearn metrics inside the function\n",
    "\n",
    "# Basic pattern\n",
    "# def function_name(param1, param2):\n",
    "    # calculations\n",
    "#    return {'key1': value1, 'key2': value2}\n",
    "\n",
    "# Task 2: Test your function with sample data\n",
    "# Print results formatted nicely\n",
    "\n",
    "\n",
    "\n",
    "def confusion_matrix_results(true, pred):\n",
    "    accuracy = accuracy_score(true, pred)\n",
    "    precision = precision_score(true, pred)\n",
    "    recall = recall_score(true, pred)\n",
    "    f1 = f1_score(true, pred)\n",
    "    return {\n",
    "        \"accuracy\": round(accuracy, 3), \n",
    "        \"precision\": round(precision, 3), \n",
    "        \"recall\": round(recall, 3), \n",
    "        \"f1\": round(f1, 3)\n",
    "    }\n",
    "\n",
    "confusion_matrix_results(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0287434b-e2a2-413a-b51f-8cba471feb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956\n",
      "\n",
      "Recall: 0.986\n",
      "\n",
      "Precision: 0.946\n",
      "\n",
      "F1 Score: 0.966\n"
     ]
    }
   ],
   "source": [
    "# Task: From memory, complete this workflow:\n",
    "# 1. Import confusion_matrix and all 4 metric functions\n",
    "# 2. Create confusion matrix from y_test, predictions\n",
    "# 3. Extract TN, FP, FN, TP using .ravel()\n",
    "# 4. Calculate all 4 metrics\n",
    "# 5. Print formatted results\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"\\nRecall: {recall:.3f}\")\n",
    "print(f\"\\nPrecision: {precision:.3f}\")\n",
    "print(f\"\\nF1 Score: {f1:.3f}\")\n",
    "\n",
    "# Write imports without looking:\n",
    "# from sklearn.metrics import ___\n",
    "\n",
    "# Verbal check:\n",
    "# - Precision answers what question?\n",
    "# - Recall answers what question?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dbdb4da-40e2-4579-a57a-875425b3e3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target  count  percentage\n",
      "0       0    212       37.26\n",
      "1       1    357       62.74\n"
     ]
    }
   ],
   "source": [
    "# -- Pattern: Percentage calculation\n",
    "# SELECT category,\n",
    "#       COUNT(*) as count,\n",
    "#       ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "# FROM table\n",
    "# GROUP BY category\n",
    "\n",
    "# -- Key syntax:\n",
    "# -- ROUND(value, 2)  -- 2 decimal places\n",
    "# -- 100.0 not 100    -- forces float division (avoids integer division trap)\n",
    "# -- SUM(...) OVER()  -- window function for total across all rows\n",
    "\n",
    "# -- Verbal tasks:\n",
    "# -- 1. Why 100.0 instead of 100?\n",
    "# -- 2. What does OVER() with empty parentheses mean?\n",
    "\n",
    "import sqlite3 as sql\n",
    "\n",
    "conn = sql.connect(\":memory:\")\n",
    "df.to_sql('breast_cancer', conn, index=False)\n",
    "\n",
    "query = \"\"\" Select \"target\",\n",
    "    COUNT(*) as count,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM breast_cancer\n",
    "    GROUP BY \"target\"\n",
    "    \"\"\"\n",
    "result = pd.read_sql(query, conn)\n",
    "print(result.head(5))\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629ab58-e6cb-4af5-99d1-76efef37ffba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
